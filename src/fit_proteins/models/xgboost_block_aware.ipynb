{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5ae286",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.sparse import load_npz\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff2d38a",
   "metadata": {},
   "source": [
    "## 2. File Path Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../../../data/processed\")\n",
    "\n",
    "def ensure_exists(path):\n",
    "    \"\"\"Check file existence and raise a clear error if missing.\"\"\"\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing required file: {path!s}. \"\n",
    "            \"Place it under data/processed or update the path.\"\n",
    "        )\n",
    "\n",
    "X_train_full_fp = DATA_DIR / \"X_train_full.npz\"\n",
    "X_test_fp       = DATA_DIR / \"X_test.npz\"\n",
    "y_train_fp      = DATA_DIR / \"y_train_full.npy\"\n",
    "ids_train_fp    = DATA_DIR / \"ids_train_full.npy\"\n",
    "ids_test_fp     = DATA_DIR / \"ids_test.npy\"\n",
    "splits_fp       = DATA_DIR / \"train_brd4_50k_clean_blocks.parquet\"\n",
    "\n",
    "for p in (X_train_full_fp, X_test_fp, y_train_fp, ids_train_fp, ids_test_fp, splits_fp):\n",
    "    ensure_exists(p)\n",
    "\n",
    "print(\"✓ All required files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc8275",
   "metadata": {},
   "source": [
    "## 3. Load Features, Labels, IDs, and Split Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c4f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = load_npz(X_train_full_fp)\n",
    "X_test       = load_npz(X_test_fp)\n",
    "\n",
    "y_train_full   = np.load(y_train_fp)\n",
    "ids_train_full = np.load(ids_train_fp)\n",
    "ids_test       = np.load(ids_test_fp)\n",
    "\n",
    "splits_df = pd.read_parquet(splits_fp)\n",
    "\n",
    "print(\"X_train_full:\", X_train_full.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train_full:\", y_train_full.shape)\n",
    "print(\"splits columns:\", splits_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0882f006",
   "metadata": {},
   "source": [
    "## 4. Apply Official Block-Aware Split\n",
    "\n",
    "**train_in**: Molecules for training\n",
    "\n",
    "**val_ood**: Out-of-distribution validation (different building blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42febce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": ids_train_full})\n",
    "df = df.merge(splits_df[[\"id\", \"split_group\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "train_mask = df[\"split_group\"] == \"train_in\"\n",
    "val_mask   = df[\"split_group\"] == \"val_ood\"\n",
    "\n",
    "X_tr = X_train_full[train_mask.values]\n",
    "y_tr = y_train_full[train_mask.values]\n",
    "\n",
    "X_val = X_train_full[val_mask.values]\n",
    "y_val = y_train_full[val_mask.values]\n",
    "\n",
    "print(\"train_in:\", X_tr.shape, \"| positives:\", y_tr.sum())\n",
    "print(\"val_ood:\", X_val.shape, \"| positives:\", y_val.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30f7a3",
   "metadata": {},
   "source": [
    "## 5. Calculate Class Imbalance Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pos = int((y_tr == 1).sum())\n",
    "N_neg = int((y_tr == 0).sum())\n",
    "scale_pos_weight = N_neg / N_pos\n",
    "\n",
    "print(\"scale_pos_weight (train_in):\", scale_pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d0f20",
   "metadata": {},
   "source": [
    "## 6. STEP 1 — Train XGBoost on train_in Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.6,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric=\"aucpr\"\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_tr, y_tr)\n",
    "print(\"XGBoost trained on train_in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257cbc6",
   "metadata": {},
   "source": [
    "## 7. STEP 2 — Validate on val_ood (Unseen Building Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ada45",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_proba_xgb = xgb_clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "ap_val_xgb  = average_precision_score(y_val, val_proba_xgb)\n",
    "roc_val_xgb = roc_auc_score(y_val, val_proba_xgb)\n",
    "\n",
    "print(f\"XGBoost – Val_OOD AP:  {ap_val_xgb:.6f}\")\n",
    "print(f\"XGBoost – Val_OOD AUC: {roc_val_xgb:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eeebc9",
   "metadata": {},
   "source": [
    "## 8. STEP 3 — Retrain XGBoost on ALL Data (train_in + val_ood)\n",
    "\n",
    "This final model will be used for predicting X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_final = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.6,\n",
    "    scale_pos_weight=scale_pos_weight,   # still computed from train_in\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric=\"aucpr\"\n",
    ")\n",
    "\n",
    "xgb_final.fit(X_train_full, y_train_full)\n",
    "print(\"XGBoost retrained on FULL training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e13e3",
   "metadata": {},
   "source": [
    "## 9. Generate Test Predictions and Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420abec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data using the final model\n",
    "test_proba = xgb_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": ids_test,\n",
    "    \"binds\": test_proba.astype(float)\n",
    "})\n",
    "\n",
    "# Create submission directory if it doesn't exist\n",
    "submission_dir = Path(\"../../../data/submission_of_models\")\n",
    "submission_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "submission_path = submission_dir / \"submission_xgboost.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✓ Submission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
