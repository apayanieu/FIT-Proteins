{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: imports & setup (PyTorch Geometric + RDKit)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch_geometric.nn import GINEConv, global_add_pool, global_mean_pool\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/pabloperezgonzalez/F.I.T-PROTEINS-NEW\n",
      "Train shape: (50000, 7)\n",
      "Test shape : (50000, 6)\n",
      "Train columns: ['id', 'protein_name', 'molecule_smiles', 'buildingblock1_smiles', 'buildingblock2_smiles', 'buildingblock3_smiles', 'binds']\n",
      "\n",
      "Using:\n",
      "  SMILES_COL = molecule_smiles\n",
      "  LABEL_COL  = binds dtype: uint8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_smiles</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCCOCC(=C)C)nc(Nc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCCC2OCCC2(C)C)nc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCCC(C)(O)CC)nc(N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCCCOCC)nc(NCC2CC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCc2nnc(C(C)(C)C)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     molecule_smiles  binds\n",
       "0  C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCCOCC(=C)C)nc(Nc...      0\n",
       "1  C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCCC2OCCC2(C)C)nc...      0\n",
       "2  C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCCC(C)(O)CC)nc(N...      0\n",
       "3  C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCCCOCC)nc(NCC2CC...      0\n",
       "4  C#CC[C@@H](CC(=O)N[Dy])Nc1nc(NCc2nnc(C(C)(C)C)...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 2: locate project root, load parquet data, configure columns\n",
    "\n",
    "candidate_roots = [\".\", \"..\", \"../..\"]\n",
    "project_root = None\n",
    "for cand in candidate_roots:\n",
    "    train_candidate = os.path.join(cand, \"data\", \"train_brd4_50k_stratified.parquet\")\n",
    "    if os.path.exists(train_candidate):\n",
    "        project_root = os.path.abspath(cand)\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find 'data/train_brd4_50k_stratified.parquet' from current directory.\"\n",
    "    )\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "\n",
    "train_path = os.path.join(project_root, \"data\", \"train_brd4_50k_stratified.parquet\")\n",
    "test_path  = os.path.join(project_root, \"data\", \"test_brd4_50k.parquet\")\n",
    "\n",
    "train_df = pd.read_parquet(train_path)\n",
    "test_df  = pd.read_parquet(test_path)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape :\", test_df.shape)\n",
    "print(\"Train columns:\", list(train_df.columns))\n",
    "\n",
    "# Your dataset\n",
    "SMILES_COL = \"molecule_smiles\"\n",
    "LABEL_COL  = \"binds\"\n",
    "\n",
    "assert SMILES_COL in train_df.columns, f\"{SMILES_COL} not in train_df\"\n",
    "assert LABEL_COL  in train_df.columns, f\"{LABEL_COL} not in train_df\"\n",
    "\n",
    "print(\"\\nUsing:\")\n",
    "print(\"  SMILES_COL =\", SMILES_COL)\n",
    "print(\"  LABEL_COL  =\", LABEL_COL, \"dtype:\", train_df[LABEL_COL].dtype)\n",
    "\n",
    "display(train_df[[SMILES_COL, LABEL_COL]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NODE_FEAT_DIM: 13\n",
      "EDGE_FEAT_DIM: 7\n",
      "Data(x=[33, 13], edge_index=[2, 68], edge_attr=[68, 7], y=[1])\n",
      "x shape       : torch.Size([33, 13])\n",
      "edge_index    : torch.Size([2, 68])\n",
      "edge_attr     : torch.Size([68, 7])\n",
      "y             : tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: atom/bond features and SMILES â†’ PyG Data\n",
    "\n",
    "# common atom and bond types; final slot in one-hot is \"other\"\n",
    "ATOM_TYPES = [\"H\", \"C\", \"N\", \"O\", \"F\", \"P\", \"S\", \"Cl\", \"Br\", \"I\"]\n",
    "BOND_TYPES = [\n",
    "    Chem.rdchem.BondType.SINGLE,\n",
    "    Chem.rdchem.BondType.DOUBLE,\n",
    "    Chem.rdchem.BondType.TRIPLE,\n",
    "    Chem.rdchem.BondType.AROMATIC,\n",
    "]\n",
    "\n",
    "def one_hot_with_other(x, choices):\n",
    "    v = [0] * (len(choices) + 1)\n",
    "    if x in choices:\n",
    "        v[choices.index(x)] = 1\n",
    "    else:\n",
    "        v[-1] = 1\n",
    "    return v\n",
    "\n",
    "def atom_to_feature_vector(atom: Chem.rdchem.Atom):\n",
    "    atom_type = one_hot_with_other(atom.GetSymbol(), ATOM_TYPES)\n",
    "    formal_charge = atom.GetFormalCharge()\n",
    "    is_aromatic = int(atom.GetIsAromatic())\n",
    "    return torch.tensor(atom_type + [formal_charge, is_aromatic], dtype=torch.float)\n",
    "\n",
    "def bond_to_feature_vector(bond: Chem.rdchem.Bond):\n",
    "    bt = bond.GetBondType()\n",
    "    bond_type_oh = one_hot_with_other(bt, BOND_TYPES)\n",
    "    is_conjugated = int(bond.GetIsConjugated())\n",
    "    is_in_ring = int(bond.IsInRing())\n",
    "    return torch.tensor(bond_type_oh + [is_conjugated, is_in_ring], dtype=torch.float)\n",
    "\n",
    "NODE_FEAT_DIM = len(ATOM_TYPES) + 1 + 2   # atom types + other + charge + aromatic\n",
    "EDGE_FEAT_DIM = len(BOND_TYPES) + 1 + 2   # bond types + other + conjugated + in_ring\n",
    "\n",
    "print(\"NODE_FEAT_DIM:\", NODE_FEAT_DIM)\n",
    "print(\"EDGE_FEAT_DIM:\", EDGE_FEAT_DIM)\n",
    "\n",
    "\n",
    "def smiles_to_data(smiles: str, y_value=None):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    Chem.SanitizeMol(mol)\n",
    "\n",
    "    # node features\n",
    "    x_list = [atom_to_feature_vector(atom) for atom in mol.GetAtoms()]\n",
    "    if len(x_list) == 0:\n",
    "        return None\n",
    "    x = torch.stack(x_list, dim=0)  # [num_nodes, NODE_FEAT_DIM]\n",
    "\n",
    "    # edges (undirected)\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        e = bond_to_feature_vector(bond)\n",
    "\n",
    "        edge_index_list.append([i, j])\n",
    "        edge_attr_list.append(e)\n",
    "\n",
    "        edge_index_list.append([j, i])\n",
    "        edge_attr_list.append(e)\n",
    "\n",
    "    if len(edge_index_list) == 0:\n",
    "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.zeros((0, EDGE_FEAT_DIM), dtype=torch.float)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.stack(edge_attr_list, dim=0)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    if y_value is not None:\n",
    "        data.y = torch.tensor([float(y_value)], dtype=torch.float)\n",
    "    return data\n",
    "\n",
    "\n",
    "# quick sanity check on 1 molecule\n",
    "example_smiles = train_df[SMILES_COL].iloc[0]\n",
    "example_label  = train_df[LABEL_COL].iloc[0]\n",
    "\n",
    "example_data = smiles_to_data(example_smiles, example_label)\n",
    "\n",
    "print(example_data)\n",
    "print(\"x shape       :\", example_data.x.shape)\n",
    "print(\"edge_index    :\", example_data.edge_index.shape)\n",
    "print(\"edge_attr     :\", example_data.edge_attr.shape)\n",
    "print(\"y             :\", example_data.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graphs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:23<00:00, 349.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 50000 graphs from 50000 rows (with labels).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building graphs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [02:50<00:00, 292.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 50000 graphs from 50000 rows (without labels).\n",
      "Train graphs: 40000\n",
      "Val graphs  : 10000\n",
      "Test graphs : 50000\n",
      "DataBatch(x=[9783, 13], edge_index=[2, 20716], edge_attr=[20716, 7], y=[256], batch=[9783], ptr=[257])\n",
      "Batch x shape      : torch.Size([9783, 13])\n",
      "Batch edge_attr    : torch.Size([20716, 7])\n",
      "Batch y shape      : torch.Size([256])\n",
      "num_graphs in batch: 256\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: PyTorch Dataset wrapping PyG Data objects + DataLoaders\n",
    "\n",
    "class BRD4MoleculeDataset(Dataset):\n",
    "    def __init__(self, df, smiles_col, label_col=None):\n",
    "        self.graphs = []\n",
    "        self.has_labels = label_col is not None\n",
    "\n",
    "        iterator = zip(\n",
    "            df[smiles_col],\n",
    "            df[label_col] if label_col is not None else [None] * len(df),\n",
    "        )\n",
    "\n",
    "        for smiles, label in tqdm(iterator, total=len(df), desc=\"Building graphs\"):\n",
    "            if label_col is not None and pd.isna(label):\n",
    "                continue\n",
    "            g = smiles_to_data(smiles, label if label_col is not None else None)\n",
    "            if g is not None:\n",
    "                self.graphs.append(g)\n",
    "\n",
    "        print(\n",
    "            f\"Built {len(self.graphs)} graphs from {len(df)} rows \"\n",
    "            f\"({'with' if self.has_labels else 'without'} labels).\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "\n",
    "train_dataset_full = BRD4MoleculeDataset(train_df, SMILES_COL, LABEL_COL)\n",
    "test_dataset       = BRD4MoleculeDataset(test_df,  SMILES_COL, label_col=None)\n",
    "\n",
    "# 80/20 train/val split\n",
    "indices = np.arange(len(train_dataset_full))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.8 * len(indices))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_dataset = Subset(train_dataset_full, train_idx)\n",
    "val_dataset   = Subset(train_dataset_full, val_idx)\n",
    "\n",
    "print(\"Train graphs:\", len(train_dataset))\n",
    "print(\"Val graphs  :\", len(val_dataset))\n",
    "print(\"Test graphs :\", len(test_dataset))\n",
    "\n",
    "BATCH_SIZE = 256  # adjust if you run out of memory\n",
    "\n",
    "train_loader = GeoDataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = GeoDataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = GeoDataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# sanity-check one batch\n",
    "batch_example = next(iter(train_loader))\n",
    "print(batch_example)\n",
    "print(\"Batch x shape      :\", batch_example.x.shape)\n",
    "print(\"Batch edge_attr    :\", batch_example.edge_attr.shape)\n",
    "print(\"Batch y shape      :\", batch_example.y.shape)\n",
    "print(\"num_graphs in batch:\", batch_example.num_graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_in_dim: 13 | edge_in_dim: 7\n",
      "GNNWithVirtualNode(\n",
      "  (node_encoder): Linear(in_features=13, out_features=128, bias=True)\n",
      "  (edge_encoder): Linear(in_features=7, out_features=128, bias=True)\n",
      "  (convs): ModuleList(\n",
      "    (0-3): 4 x GINEConv(nn=Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0-3): 4 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (virtualnode_embedding): Embedding(1, 128)\n",
      "  (mlp_virtual_list): ModuleList(\n",
      "    (0-2): 3 x Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp_readout): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CELL 5 (UPDATED): focal loss + improved GNN with sum+mean pooling\n",
    "\n",
    "def binary_focal_loss_with_logits(\n",
    "    logits,\n",
    "    targets,\n",
    "    alpha: float = 0.99,   # more emphasis on positives\n",
    "    gamma: float = 1.5,    # slightly less peaky than 2.0\n",
    "    reduction: str = \"mean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Focal loss for severe class imbalance.\n",
    "    alpha is the weight for the positive class.\n",
    "    \"\"\"\n",
    "    logits = logits.view(-1)\n",
    "    targets = targets.view(-1).float()\n",
    "\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "    p_t = torch.exp(-bce)\n",
    "    loss = alpha * (1 - p_t) ** gamma * bce\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "\n",
    "class GNNWithVirtualNode(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_dim: int,\n",
    "        edge_in_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 4,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.node_encoder = nn.Linear(node_in_dim, hidden_dim)\n",
    "        self.edge_encoder = nn.Linear(edge_in_dim, hidden_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns   = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "            )\n",
    "            conv = GINEConv(mlp)  # GIN with edge features\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # virtual node embedding\n",
    "        self.virtualnode_embedding = nn.Embedding(1, hidden_dim)\n",
    "        nn.init.constant_(self.virtualnode_embedding.weight.data, 0.0)\n",
    "\n",
    "        # MLPs to update virtual node after each layer (except last)\n",
    "        self.mlp_virtual_list = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.mlp_virtual_list.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # graph-level readout MLP\n",
    "        # note: 2*hidden_dim input because we concat [sum || mean] pooling\n",
    "        self.mlp_readout = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x,\n",
    "            data.edge_index,\n",
    "            data.edge_attr,\n",
    "            data.batch,\n",
    "        )\n",
    "\n",
    "        x = self.node_encoder(x)\n",
    "        edge_attr = self.edge_encoder(edge_attr)\n",
    "\n",
    "        num_graphs = batch.max().item() + 1\n",
    "        virtualnode_emb = self.virtualnode_embedding.weight.repeat(num_graphs, 1)\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            # add virtual node embedding to node features\n",
    "            x = x + virtualnode_emb[batch]\n",
    "\n",
    "            x = self.convs[layer](x, edge_index, edge_attr)\n",
    "            x = self.bns[layer](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "            # update virtual node (except last layer)\n",
    "            if layer < self.num_layers - 1:\n",
    "                pooled = global_add_pool(x, batch)  # [num_graphs, hidden_dim]\n",
    "                virtualnode_emb = virtualnode_emb + self.mlp_virtual_list[layer](pooled)\n",
    "\n",
    "        # global pooling: use both sum and mean\n",
    "        graph_sum  = global_add_pool(x, batch)           # [num_graphs, hidden_dim]\n",
    "        graph_mean = global_mean_pool(x, batch)          # [num_graphs, hidden_dim]\n",
    "        graph_emb  = torch.cat([graph_sum, graph_mean], dim=1)  # [num_graphs, 2H]\n",
    "\n",
    "        logits = self.mlp_readout(graph_emb).view(-1)    # [num_graphs]\n",
    "        return logits\n",
    "\n",
    "\n",
    "# instantiate model (same as before)\n",
    "sample = train_dataset_full[0]\n",
    "node_in_dim = sample.x.size(1)\n",
    "edge_in_dim = sample.edge_attr.size(1)\n",
    "\n",
    "print(\"node_in_dim:\", node_in_dim, \"| edge_in_dim:\", edge_in_dim)\n",
    "\n",
    "model = GNNWithVirtualNode(\n",
    "    node_in_dim=node_in_dim,\n",
    "    edge_in_dim=edge_in_dim,\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    dropout=0.2,\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss: 0.0245 | val loss: 0.7273 | val ROC-AUC: 0.5690 | val AP: 0.0066\n",
      "\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train loss: 0.0147 | val loss: 0.0700 | val ROC-AUC: 0.7893 | val AP: 0.0221\n",
      "\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train loss: 0.0132 | val loss: 0.0805 | val ROC-AUC: 0.8040 | val AP: 0.0515\n",
      "\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train loss: 0.0118 | val loss: 0.0490 | val ROC-AUC: 0.7696 | val AP: 0.0717\n",
      "\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train loss: 0.0118 | val loss: 0.0328 | val ROC-AUC: 0.8018 | val AP: 0.0288\n",
      "\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | train loss: 0.0111 | val loss: 0.0588 | val ROC-AUC: 0.8426 | val AP: 0.0370\n",
      "\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | train loss: 0.0110 | val loss: 0.0697 | val ROC-AUC: 0.8541 | val AP: 0.0874\n",
      "\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | train loss: 0.0113 | val loss: 0.0728 | val ROC-AUC: 0.8697 | val AP: 0.0623\n",
      "\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | train loss: 0.0114 | val loss: 0.0644 | val ROC-AUC: 0.8510 | val AP: 0.0753\n",
      "\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train loss: 0.0111 | val loss: 0.0615 | val ROC-AUC: 0.9022 | val AP: 0.1131\n",
      "\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | train loss: 0.0099 | val loss: 0.0836 | val ROC-AUC: 0.8933 | val AP: 0.0799\n",
      "\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | train loss: 0.0103 | val loss: 0.0708 | val ROC-AUC: 0.8540 | val AP: 0.0613\n",
      "\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | train loss: 0.0100 | val loss: 0.0460 | val ROC-AUC: 0.8411 | val AP: 0.0524\n",
      "\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | train loss: 0.0094 | val loss: 0.0445 | val ROC-AUC: 0.9172 | val AP: 0.1644\n",
      "\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | train loss: 0.0086 | val loss: 0.1032 | val ROC-AUC: 0.9111 | val AP: 0.1119\n",
      "\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | train loss: 0.0086 | val loss: 0.0392 | val ROC-AUC: 0.9046 | val AP: 0.1336\n",
      "\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | train loss: 0.0088 | val loss: 0.0410 | val ROC-AUC: 0.8942 | val AP: 0.1490\n",
      "\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | train loss: 0.0082 | val loss: 0.0471 | val ROC-AUC: 0.9162 | val AP: 0.1693\n",
      "\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | train loss: 0.0079 | val loss: 0.0373 | val ROC-AUC: 0.9163 | val AP: 0.1775\n",
      "Early stopping: no ROC-AUC improvement for 5 epochs.\n",
      "\n",
      "Best validation ROC-AUC: 0.9172, AP: 0.1644 at epoch 14\n"
     ]
    }
   ],
   "source": [
    "# CELL 6 (FIXED): longer training + LR scheduler + early stopping on ROC-AUC\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, alpha=0.99, gamma=1.5):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_graphs = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(batch)\n",
    "        targets = batch.y.view(-1)\n",
    "\n",
    "        loss = binary_focal_loss_with_logits(\n",
    "            logits,\n",
    "            targets,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "        n_graphs += batch.num_graphs\n",
    "\n",
    "    return total_loss / n_graphs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_graphs = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Val\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch)\n",
    "        targets = batch.y.view(-1)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"sum\")\n",
    "        total_loss += loss.item()\n",
    "        n_graphs += batch.num_graphs\n",
    "\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_targets)\n",
    "    y_prob = np.concatenate(all_probs)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    return total_loss / n_graphs, roc_auc, ap\n",
    "\n",
    "\n",
    "MAX_EPOCHS = 30       # upper bound; early stopping will usually stop earlier\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",      # maximize ROC-AUC\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "best_val_auc = -float(\"inf\")\n",
    "best_val_ap = 0.0\n",
    "best_epoch = 0\n",
    "no_improve = 0\n",
    "\n",
    "print(\"Training on device:\", device)\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{MAX_EPOCHS}\")\n",
    "    # ðŸ”§ FIX: pass optimizer into train_one_epoch\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_auc, val_ap = evaluate(model, val_loader)\n",
    "\n",
    "    scheduler.step(val_auc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train loss: {train_loss:.4f} | \"\n",
    "        f\"val loss: {val_loss:.4f} | \"\n",
    "        f\"val ROC-AUC: {val_auc:.4f} | \"\n",
    "        f\"val AP: {val_ap:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_auc > best_val_auc + 1e-4:\n",
    "        best_val_auc = val_auc\n",
    "        best_val_ap = val_ap\n",
    "        best_epoch = epoch\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve >= 5:\n",
    "        print(f\"Early stopping: no ROC-AUC improvement for {no_improve} epochs.\")\n",
    "        break\n",
    "\n",
    "print(\n",
    "    f\"\\nBest validation ROC-AUC: {best_val_auc:.4f}, \"\n",
    "    f\"AP: {best_val_ap:.4f} at epoch {best_epoch}\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Metrics on val_full set ===\n",
      "Prevalence (mean label) : 0.0045\n",
      "ROC-AUC                 : 0.8493\n",
      "Average Precision (AP)  : 0.0520\n",
      "Best F1                 : 0.1446 at threshold 0.354\n",
      "F1 at threshold 0.5     : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: detailed metrics on validation set (AP, ROC-AUC, F1, best threshold)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics_pyg(model, loader, name=\"val\"):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        y = batch.y.view(-1).cpu().numpy()\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_targets.append(y)\n",
    "\n",
    "    y_true = np.concatenate(all_targets)\n",
    "    y_prob = np.concatenate(all_probs)\n",
    "\n",
    "    prevalence = float(y_true.mean())\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y_true, y_prob)\n",
    "    f1_scores = 2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-8)\n",
    "    best_idx = int(np.argmax(f1_scores))\n",
    "    best_thr = float(thr[best_idx])\n",
    "    best_f1 = float(f1_scores[best_idx])\n",
    "\n",
    "    f1_at_05 = float(f1_score(y_true, (y_prob >= 0.5).astype(int)))\n",
    "\n",
    "    print(f\"=== Metrics on {name} set ===\")\n",
    "    print(f\"Prevalence (mean label) : {prevalence:.4f}\")\n",
    "    print(f\"ROC-AUC                 : {roc_auc:.4f}\")\n",
    "    print(f\"Average Precision (AP)  : {ap:.4f}\")\n",
    "    print(f\"Best F1                 : {best_f1:.4f} at threshold {best_thr:.3f}\")\n",
    "    print(f\"F1 at threshold 0.5     : {f1_at_05:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"prevalence\": prevalence,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"ap\": ap,\n",
    "        \"best_f1\": best_f1,\n",
    "        \"best_thr\": best_thr,\n",
    "        \"f1_at_0.5\": f1_at_05,\n",
    "    }\n",
    "\n",
    "metrics_val = compute_metrics_pyg(model, val_loader, name=\"val_full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CELL 7: detailed metrics on validation set (AP, ROC-AUC, F1, best threshold)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_metrics_pyg\u001b[39m(model, loader, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m     all_probs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# CELL 7: detailed metrics on validation set (AP, ROC-AUC, F1, best threshold)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics_pyg(model, loader, name=\"val\"):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        y = batch.y.view(-1).cpu().numpy()\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_targets.append(y)\n",
    "\n",
    "    y_true = np.concatenate(all_targets)\n",
    "    y_prob = np.concatenate(all_probs)\n",
    "\n",
    "    prevalence = float(y_true.mean())\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y_true, y_prob)\n",
    "    f1_scores = 2 * prec[:-1] * rec[:-1] / (prec[:-1] + rec[:-1] + 1e-8)\n",
    "    best_idx = int(np.argmax(f1_scores))\n",
    "    best_thr = float(thr[best_idx])\n",
    "    best_f1 = float(f1_scores[best_idx])\n",
    "\n",
    "    f1_at_05 = float(f1_score(y_true, (y_prob >= 0.5).astype(int)))\n",
    "\n",
    "    print(f\"=== Metrics on {name} set ===\")\n",
    "    print(f\"Prevalence (mean label) : {prevalence:.4f}\")\n",
    "    print(f\"ROC-AUC                 : {roc_auc:.4f}\")\n",
    "    print(f\"Average Precision (AP)  : {ap:.4f}\")\n",
    "    print(f\"Best F1                 : {best_f1:.4f} at threshold {best_thr:.3f}\")\n",
    "    print(f\"F1 at threshold 0.5     : {f1_at_05:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"prevalence\": prevalence,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"ap\": ap,\n",
    "        \"best_f1\": best_f1,\n",
    "        \"best_thr\": best_thr,\n",
    "        \"f1_at_0.5\": f1_at_05,\n",
    "    }\n",
    "\n",
    "metrics_val = compute_metrics_pyg(model, val_loader, name=\"val_full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions shape: (50000,)\n",
      "Saved predictions to: /Users/pabloperezgonzalez/F.I.T-PROTEINS-NEW/notebooks/neural_networks/artifacts/gnn_pyg_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.309667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.005264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.212053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.331305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.039361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    y_pred\n",
       "0   0  0.309667\n",
       "1   1  0.005264\n",
       "2   2  0.212053\n",
       "3   3  0.331305\n",
       "4   4  0.039361"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL 8 (optional): predict on test set and save CSV\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_loader_pyg(model, loader):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        logits = model(batch)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "    return np.concatenate(all_probs)\n",
    "\n",
    "test_preds = predict_loader_pyg(model, test_loader)\n",
    "print(\"Test predictions shape:\", test_preds.shape)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": np.arange(len(test_preds)),\n",
    "    \"y_pred\": test_preds,\n",
    "})\n",
    "\n",
    "artifact_dir = os.path.join(project_root, \"notebooks\", \"neural_networks\", \"artifacts\")\n",
    "os.makedirs(artifact_dir, exist_ok=True)\n",
    "save_path = os.path.join(artifact_dir, \"gnn_pyg_predictions.csv\")\n",
    "submission.to_csv(save_path, index=False)\n",
    "print(\"Saved predictions to:\", save_path)\n",
    "\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fit-proteins-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
